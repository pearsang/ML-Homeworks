{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data from the arff file and converting it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.arff import loadarff\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = loadarff('./column_diagnosis.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "df['class'] = df['class'].str.decode('utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating the input data and the output data, required for some sklean functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop('class', axis=1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the discriminative power of each feature in accordance with f_classif criterion. The higher the value, the more discriminative the feature is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "fimportance = f_classif(x, y)\n",
    "\n",
    "scores = fimportance[0]\n",
    "\n",
    "highest_discriminative_power = x.columns.values[scores.argmax()]\n",
    "lowest_discriminative_power = x.columns.values[scores.argmin()]\n",
    "print('Input variable with highest discriminative power: ', highest_discriminative_power)\n",
    "print('Input variable with lowest discriminative power: ', lowest_discriminative_power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the class-conditional probability density functions of these two input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the class-conditional probability density functions of these two input variables.\n",
    "\n",
    "sns.displot(data=df, x=highest_discriminative_power, kind='kde', hue='class', fill=True)\n",
    "plt.title('Class-conditional probability density function for ' + highest_discriminative_power)\n",
    "plt.show()\n",
    "\n",
    "sns.displot(data=df, x=lowest_discriminative_power, kind='kde', hue='class', fill=True)\n",
    "plt.title('Class-conditional probability density function for ' + lowest_discriminative_power)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will split the data into training and testing sets. We will use 70% of the data for training and 30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics, tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Split the data into training and test sets\n",
    "avg_train_accs, avg_test_accs = [], []\n",
    "# Define the depth limits\n",
    "depth_limits = [1,2,3,4,5,6,8,10]\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the training data to train the model. We will use the DecisionTreeClassifier from sklearn.tree to train the model. We will be doing 10 runs of the model and will be averaging the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "for i in depth_limits:\n",
    "    train_accs, test_accs = [], []\n",
    "    for j in range(n_runs):\n",
    "        # train classifier    \n",
    "        predictor = tree.DecisionTreeClassifier(max_depth=i, random_state=0)\n",
    "        # fit classifier\n",
    "        predictor.fit(X_train, y_train)\n",
    "        # test classifier\n",
    "        y_pred_test = predictor.predict(X_test)\n",
    "        y_pred_train = predictor.predict(X_train)\n",
    "        \n",
    "        # calculate accuracy\n",
    "        train_acc = round(metrics.accuracy_score(y_train, y_pred_train),2)        \n",
    "        test_acc = round(metrics.accuracy_score(y_test, y_pred_test),2)\n",
    "        \n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "    \n",
    "    avg_train_accs.append(np.mean(train_accs))\n",
    "    avg_test_accs.append(np.mean(test_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess in a single plot both the training and testing accuracies of a decision tree with depth limits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=depth_limits, y=avg_train_accs, label='Training Accuracy')\n",
    "sns.lineplot(x=depth_limits, y=avg_test_accs, label='Testing Accuracy')\n",
    "sns.scatterplot(x=depth_limits, y=avg_train_accs)\n",
    "sns.scatterplot(x=depth_limits, y=avg_test_accs)\n",
    "plt.grid()\n",
    "plt.xlabel('Depth Limit')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Testing Accuracy for Decision Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the results from Question 2, we can observe the relationship between the max depth of the decision tree and its training and testing accuracies. We can see some key points from the plot:\n",
    "- As the max depth of the decision tree increases, the training accuracy generally improves. This is expected because a deeper tree can fit the training data better, and hence, the training accuracy will improve.\n",
    "  \n",
    "- When analyzing the testing accuracy, it shows a different trend. Initially, as the tree uses a smaller depth limits, the testing accuracy improves. But, after a certain point (around depth of 5), the testing accuracy starts to decline. This is a clear sign of overfitting. The model is overfitting the training data and hence, the testing accuracy is declining.\n",
    "  \n",
    "- Considering the prior analysis, the optimal max depth for the decision tree is 5. Beyond that point, the model tens to overfit the training data and hence, the testing accuracy starts to decline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Decision Tree Classifier with the specified parameters\n",
    "predictor = tree.DecisionTreeClassifier(random_state=0, min_samples_leaf=20)\n",
    "\n",
    "# Fit the classifier on all available data\n",
    "predictor.fit(x, y)\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "tree.plot_tree(predictor, filled=True, feature_names=x.columns.values, class_names=y, rounded=True)\n",
    "plt.title(\"Decision Tree Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
